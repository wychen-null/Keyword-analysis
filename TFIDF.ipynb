{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T07:21:57.132980Z",
     "start_time": "2023-11-25T07:21:57.127745Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/dingyi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "from nltk.text import TextCollection\n",
    "from nltk.tokenize import word_tokenize\n",
    "import json\n",
    "import nltk\n",
    "import csv\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag\n",
    "\n",
    "nltk.download('wordnet')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "32a0e42c5fa4559b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T07:21:57.151189Z",
     "start_time": "2023-11-25T07:21:57.145052Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    token_list = nltk.word_tokenize(text)\n",
    "    # tagged_words = pos_tag(token_list)\n",
    "    return token_list\n",
    "\n",
    "\n",
    "def stemmer_words(word):\n",
    "    stemmer = PorterStemmer()\n",
    "    # 对每个词进行词干还原\n",
    "    stemmed_word = stemmer.stem(word)\n",
    "    return stemmed_word\n",
    "\n",
    "\n",
    "def lemmatizer_word(word_list):\n",
    "    tagged_words = pos_tag(word_list)\n",
    "    tagged_words = clean_by_pos(tagged_words)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_words = []\n",
    "    for tag in tagged_words:\n",
    "        wordnet_pos = get_wordnet_pos(tag[1]) or wordnet.NOUN\n",
    "        lemmatized_words.append(lemmatizer.lemmatize(tag[0], pos=wordnet_pos))\n",
    "    return lemmatized_words\n",
    "\n",
    "\n",
    "# 根据词性清理词汇\n",
    "def clean_by_pos(tagged_words):\n",
    "    cleaned_tagged_word = []\n",
    "    include_tag = [\"NN\", \"NNS\", \"NNP\", \"NNPS\", \"NP\"]\n",
    "    for tagged_word in tagged_words:\n",
    "        if tagged_word[1] in include_tag:\n",
    "            cleaned_tagged_word.append(tagged_word)\n",
    "    return cleaned_tagged_word\n",
    "\n",
    "\n",
    "# 获取单词的词性\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def load_security_words():\n",
    "    nltk.download('stopwords')\n",
    "    stopwords_security_list = stopwords.words('english')\n",
    "    with open(\"other_file/glasgow_stop_words.txt\", \"r\") as f:\n",
    "        stopwords_list = f.read().split('\\n')\n",
    "        stopwords_security_list.extend(stopwords_list)\n",
    "    with open(\"other_file/info_security_stopwords.txt\", \"r\") as s:\n",
    "        info_security_list = s.read().split('\\n')\n",
    "        stopwords_security_list.extend(info_security_list)\n",
    "    stopwords_security = set(stopwords_security_list)\n",
    "    return stopwords_security\n",
    "\n",
    "\n",
    "def remove_stopwords(stopwords_security, word_list):\n",
    "    cleaned_list = []\n",
    "    # 利用词干分析过滤掉停用词\n",
    "    stem_stopwords = [stemmer_words(word) for word in stopwords_security]\n",
    "    # 词形还原\n",
    "    word_list = lemmatizer_word(word_list)\n",
    "    for word in word_list:\n",
    "        if stemmer_words(word.lower()) not in stem_stopwords:\n",
    "            cleaned_list.append(word)\n",
    "    return cleaned_list\n",
    "\n",
    "\n",
    "# load stopwords\n",
    "def load_data(year, start_month, end_month):\n",
    "    count = start_month\n",
    "    news_data = []\n",
    "    while count <= end_month:\n",
    "        with open('output/cleaned_data/cleaned_data_' + year + '_' + format_month(count) + '.json', 'r', encoding='utf-8') as file:\n",
    "            news_list = json.load(file)\n",
    "            for news_json in news_list:\n",
    "                text = news_json['title'] + '.' + news_json['text']\n",
    "                news_data.append(text.lower())\n",
    "        count = count + 1\n",
    "    print(\"Total amount of data is \" + str(len(news_data)))\n",
    "    return news_data\n",
    "\n",
    "\n",
    "def format_month(num):\n",
    "    formatted_str = str(num).zfill(2)\n",
    "    return formatted_str\n",
    "\n",
    "\n",
    "def write_file(keyword_counter, year, start_month, end_month):\n",
    "    word_dict = dict(keyword_counter)\n",
    "    sorted_data = sorted(word_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "    # 只输出前120个高频词\n",
    "    with open('output/nltk_output/output_nltk_' + year + '_' + format_month(start_month)\n",
    "              + '_' + format_month(end_month) + '_' + 'nation' + '.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(sorted_data, f, indent=4, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8c9c26bab89865a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T07:22:07.122825Z",
     "start_time": "2023-11-25T07:21:57.150957Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/dingyi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total amount of data is 358\n",
      "start nlkt process\n"
     ]
    }
   ],
   "source": [
    "year = '2023'\n",
    "start_month = 10\n",
    "end_month = 11\n",
    "\n",
    "data = load_data(year, start_month, end_month)\n",
    "keyword_list = []\n",
    "stopword_list = load_security_words()\n",
    "# sent = []\n",
    "\n",
    "print(\"start nlkt process\")\n",
    "for i, content in enumerate(data):\n",
    "    word_list = tokenize(content)\n",
    "    word_list_cleaned = remove_stopwords(stopword_list, word_list)\n",
    "    # sent.append(word_list_cleaned)\n",
    "    keyword_list.extend(word_list_cleaned)\n",
    "\n",
    "keyword_counter = Counter(keyword_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "72e9f43aea5586c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T07:22:07.142927Z",
     "start_time": "2023-11-25T07:22:07.126639Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# corpus=TextCollection(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c3353b0b6addd2ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T07:22:26.728812Z",
     "start_time": "2023-11-25T07:22:07.142421Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_dict = dict(keyword_counter)\n",
    "sorted_data = sorted(word_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "# 创建CSV文件并写入数据\n",
    "with open('output/word_freq/2023_season4.csv', 'w', newline='') as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "\n",
    "    # 写入CSV文件的表头\n",
    "    writer.writerow(['word', 'freq'])\n",
    "\n",
    "    # 写入JSON数据\n",
    "    for item in sorted_data:\n",
    "        writer.writerow([item[0], item[1]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
