{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T12:47:12.631174Z",
     "start_time": "2023-11-24T12:47:12.178191Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/dingyi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim import corpora, models\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.text import TextCollection\n",
    "from nltk.tokenize import word_tokenize\n",
    "import json\n",
    "import nltk\n",
    "import csv\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag\n",
    "\n",
    "nltk.download('wordnet')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c1fccd7fd095a61",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T12:50:12.700752Z",
     "start_time": "2023-11-24T12:49:58.953665Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    token_list = nltk.word_tokenize(text)\n",
    "    # tagged_words = pos_tag(token_list)\n",
    "    return token_list\n",
    "\n",
    "\n",
    "def stemmer_words(word):\n",
    "    stemmer = PorterStemmer()\n",
    "    # 对每个词进行词干还原\n",
    "    stemmed_word = stemmer.stem(word)\n",
    "    return stemmed_word\n",
    "\n",
    "\n",
    "def lemmatizer_word(word_list):\n",
    "    tagged_words = pos_tag(word_list)\n",
    "    tagged_words = clean_by_pos(tagged_words)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_words = []\n",
    "    for tag in tagged_words:\n",
    "        wordnet_pos = get_wordnet_pos(tag[1]) or wordnet.NOUN\n",
    "        lemmatized_words.append(lemmatizer.lemmatize(tag[0], pos=wordnet_pos))\n",
    "    return lemmatized_words\n",
    "\n",
    "\n",
    "# 根据词性清理词汇\n",
    "def clean_by_pos(tagged_words):\n",
    "    cleaned_tagged_word = []\n",
    "    include_tag = [\"NN\", \"NNS\", \"NNP\", \"NNPS\", \"NP\"]\n",
    "    for tagged_word in tagged_words:\n",
    "        if tagged_word[1] in include_tag:\n",
    "            cleaned_tagged_word.append(tagged_word)\n",
    "    return cleaned_tagged_word\n",
    "\n",
    "\n",
    "# 获取单词的词性\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def load_security_words():\n",
    "    nltk.download('stopwords')\n",
    "    stopwords_security_list = stopwords.words('english')\n",
    "    with open(\"other_file/glasgow_stop_words.txt\", \"r\") as f:\n",
    "        stopwords_list = f.read().split('\\n')\n",
    "        stopwords_security_list.extend(stopwords_list)\n",
    "    with open(\"other_file/info_security_stopwords.txt\", \"r\") as s:\n",
    "        info_security_list = s.read().split('\\n')\n",
    "        stopwords_security_list.extend(info_security_list)\n",
    "    stopwords_security = set(stopwords_security_list)\n",
    "    return stopwords_security\n",
    "\n",
    "\n",
    "def remove_stopwords(stopwords_security, word_list):\n",
    "    cleaned_list = []\n",
    "    # 利用词干分析过滤掉停用词\n",
    "    stem_stopwords = [stemmer_words(word) for word in stopwords_security]\n",
    "    # 词形还原\n",
    "    word_list = lemmatizer_word(word_list)\n",
    "    for word in word_list:\n",
    "        if stemmer_words(word.lower()) not in stem_stopwords:\n",
    "            cleaned_list.append(word)\n",
    "    return cleaned_list\n",
    "\n",
    "\n",
    "# load stopwords\n",
    "def load_data(year, start_month, end_month):\n",
    "    count = start_month\n",
    "    news_data = []\n",
    "    while count <= end_month:\n",
    "        with open('output/cleaned_data/cleaned_data_' + year + '_' + format_month(count) + '.json', 'r', encoding='utf-8') as file:\n",
    "            news_list = json.load(file)\n",
    "            for news_json in news_list:\n",
    "                text = news_json['title'] + '.' + news_json['text']\n",
    "                news_data.append(text.lower())\n",
    "        count = count + 1\n",
    "    print(\"Total amount of data is \" + str(len(news_data)))\n",
    "    return news_data\n",
    "\n",
    "\n",
    "def format_month(num):\n",
    "    formatted_str = str(num).zfill(2)\n",
    "    return formatted_str\n",
    "\n",
    "\n",
    "def write_file(keyword_counter, year, start_month, end_month):\n",
    "    word_dict = dict(keyword_counter)\n",
    "    sorted_data = sorted(word_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "    # 只输出前120个高频词\n",
    "    with open('output/nltk_output/output_nltk_' + year + '_' + format_month(start_month)\n",
    "              + '_' + format_month(end_month) + '_' + 'nation' + '.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(sorted_data, f, indent=4, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/dingyi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total amount of data is 646\n",
      "start nlkt process\n"
     ]
    }
   ],
   "source": [
    "year = '2023'\n",
    "start_month = 4\n",
    "end_month = 6\n",
    "\n",
    "data = load_data(year, start_month, end_month)\n",
    "keyword_list = []\n",
    "stopword_list = load_security_words()\n",
    "sent=[]\n",
    "\n",
    "print(\"start nlkt process\")\n",
    "for i, content in enumerate(data):\n",
    "    word_list = tokenize(content)\n",
    "    word_list_cleaned = remove_stopwords(stopword_list, word_list)\n",
    "    sent.append(word_list_cleaned)\n",
    "    keyword_list.extend(word_list_cleaned)\n",
    "\n",
    "# year = '2023'\n",
    "# start_month = 1\n",
    "# end_month = 11\n",
    "# data = load_data(year, start_month, end_month)\n",
    "# for i, content in enumerate(data):\n",
    "#     word_list = tokenize(content)\n",
    "#     word_list_cleaned = remove_stopwords(stopword_list, word_list)\n",
    "#     sent.append(word_list_cleaned)\n",
    "#     keyword_list.extend(word_list_cleaned)\n",
    "    \n",
    "keyword_counter = Counter(keyword_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bc5832f121c4555e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T06:25:17.279649Z",
     "start_time": "2023-11-25T06:25:10.360513Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #1:\n",
      "['ransomware', 'malware']\n",
      "Topic #2:\n",
      "['malware', 'ransomware']\n",
      "Topic #3:\n",
      "['email', 'malware']\n",
      "Topic #4:\n",
      "['malware', 'website']\n",
      "Topic #5:\n",
      "['snake', 'ransomware']\n"
     ]
    }
   ],
   "source": [
    "# 构建词典\n",
    "dictionary = corpora.Dictionary(sent)\n",
    "\n",
    "# 将文本转换为词袋表示\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in sent]\n",
    "\n",
    "# 训练LDA模型\n",
    "num_topics = 5  # 设置主题数\n",
    "lda_model = models.LdaModel(bow_corpus, num_topics=num_topics, id2word=dictionary, passes=10)\n",
    "\n",
    "# 输出每个主题的关键词\n",
    "for topic_id in range(num_topics):\n",
    "    print(f\"Topic #{topic_id + 1}:\")\n",
    "    topic_keywords = lda_model.show_topic(topic_id, topn=2)\n",
    "    keywords = [word for word, _ in topic_keywords]\n",
    "    print(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e16e332c9f2827f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T12:25:39.186570Z",
     "start_time": "2023-11-24T12:25:39.018955Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from gensim.models import Word2Vec\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # 准备文本数据\n",
    "# texts = [\n",
    "#     \"The COVID-19 pandemic has caused global disruptions.\",\n",
    "#     \"Efforts are being made to develop effective vaccines.\",\n",
    "#     \"Lockdown measures have been implemented to control the spread.\",\n",
    "#     \"Economic recovery is a major concern amidst the crisis.\",\n",
    "#     \"New variants of the virus are being closely monitored.\",\n",
    "#     \"Vaccination campaigns are being rolled out worldwide.\"\n",
    "# ]\n",
    "\n",
    "# # 数据预处理（分词等）\n",
    "# corpus = [text.lower().split() for text in texts]\n",
    "\n",
    "# # 训练Word2Vec模型\n",
    "# model = Word2Vec(corpus, vector_size=100, window=5, min_count=1)\n",
    "\n",
    "# # 获取与给定词语相关的热点词汇\n",
    "# target_word = \"pandemic\"\n",
    "# similar_words = model.wv.most_similar(target_word)\n",
    "\n",
    "# # 打印热点词汇\n",
    "# print(f\"Words related to '{target_word}':\")\n",
    "# for word, similarity in similar_words:\n",
    "#     print(f\"{word}: {similarity:.4f}\")\n",
    "\n",
    "# # 绘制词向量变化图\n",
    "# words_to_plot = [target_word] + [word for word, _ in similar_words]\n",
    "# vectors = [model.wv[word] for word in words_to_plot]\n",
    "\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# for i, word in enumerate(words_to_plot):\n",
    "#     plt.scatter(vectors[i][0], vectors[i][1])\n",
    "#     plt.annotate(word, (vectors[i][0], vectors[i][1]))\n",
    "\n",
    "# plt.xlabel(\"Dimension 1\")\n",
    "# plt.ylabel(\"Dimension 2\")\n",
    "# plt.title(\"Word Embeddings\")\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
