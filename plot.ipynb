{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前十词频"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/dingyi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "from nltk.text import TextCollection\n",
    "from nltk.tokenize import word_tokenize\n",
    "import json\n",
    "import nltk\n",
    "import csv\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag\n",
    "\n",
    "nltk.download('wordnet')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    token_list = nltk.word_tokenize(text)\n",
    "    # tagged_words = pos_tag(token_list)\n",
    "    return token_list\n",
    "\n",
    "\n",
    "def stemmer_words(word):\n",
    "    stemmer = PorterStemmer()\n",
    "    # 对每个词进行词干还原\n",
    "    stemmed_word = stemmer.stem(word)\n",
    "    return stemmed_word\n",
    "\n",
    "\n",
    "def lemmatizer_word(word_list):\n",
    "    tagged_words = pos_tag(word_list)\n",
    "    tagged_words = clean_by_pos(tagged_words)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_words = []\n",
    "    for tag in tagged_words:\n",
    "        wordnet_pos = get_wordnet_pos(tag[1]) or wordnet.NOUN\n",
    "        lemmatized_words.append(lemmatizer.lemmatize(tag[0], pos=wordnet_pos))\n",
    "    return lemmatized_words\n",
    "\n",
    "\n",
    "# 根据词性清理词汇\n",
    "def clean_by_pos(tagged_words):\n",
    "    cleaned_tagged_word = []\n",
    "    include_tag = [\"NN\", \"NNS\", \"NNP\", \"NNPS\", \"NP\"]\n",
    "    for tagged_word in tagged_words:\n",
    "        if tagged_word[1] in include_tag:\n",
    "            cleaned_tagged_word.append(tagged_word)\n",
    "    return cleaned_tagged_word\n",
    "\n",
    "\n",
    "# 获取单词的词性\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def load_security_words():\n",
    "    nltk.download('stopwords')\n",
    "    stopwords_security_list = stopwords.words('english')\n",
    "    with open(\"other_file/glasgow_stop_words.txt\", \"r\") as f:\n",
    "        stopwords_list = f.read().split('\\n')\n",
    "        stopwords_security_list.extend(stopwords_list)\n",
    "    with open(\"other_file/all_info_security_stopwords.txt\", \"r\") as s:\n",
    "        info_security_list = s.read().split('\\n')\n",
    "        stopwords_security_list.extend(info_security_list)\n",
    "    stopwords_security = set(stopwords_security_list)\n",
    "    return stopwords_security\n",
    "\n",
    "\n",
    "def remove_stopwords(stopwords_security, word_list):\n",
    "    cleaned_list = []\n",
    "    # 利用词干分析过滤掉停用词\n",
    "    stem_stopwords = [stemmer_words(word) for word in stopwords_security]\n",
    "    # 词形还原\n",
    "    word_list = lemmatizer_word(word_list)\n",
    "    for word in word_list:\n",
    "        if stemmer_words(word.lower()) not in stem_stopwords:\n",
    "            cleaned_list.append(word)\n",
    "    return cleaned_list\n",
    "\n",
    "\n",
    "# load stopwords\n",
    "def load_data(year, start_month, end_month):\n",
    "    count = start_month\n",
    "    news_data = []\n",
    "    while count <= end_month:\n",
    "        with open('output/cleaned_data/cleaned_data_' + year + '_' + format_month(count) + '.json', 'r', encoding='utf-8') as file:\n",
    "            news_list = json.load(file)\n",
    "            for news_json in news_list:\n",
    "                text = news_json['title'] + '.' + news_json['text']\n",
    "                news_data.append(text)\n",
    "        count = count + 1\n",
    "    print(\"Total amount of data is \" + str(len(news_data)))\n",
    "    return news_data\n",
    "\n",
    "\n",
    "def format_month(num):\n",
    "    formatted_str = str(num).zfill(2)\n",
    "    return formatted_str\n",
    "\n",
    "\n",
    "def write_file(keyword_counter, year, start_month, end_month):\n",
    "    word_dict = dict(keyword_counter)\n",
    "    sorted_data = sorted(word_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "    # 只输出前120个高频词\n",
    "    with open('output/nltk_output/output_nltk_' + year + '_' + format_month(start_month)\n",
    "              + '_' + format_month(end_month) + '_' + 'nation' + '.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(sorted_data, f, indent=4, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/dingyi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total amount of data is 1268\n",
      "start nlkt process\n",
      "Total amount of data is 2356\n"
     ]
    }
   ],
   "source": [
    "year = '2022'\n",
    "start_month = 7\n",
    "end_month = 12\n",
    "\n",
    "data = load_data(year, start_month, end_month)\n",
    "keyword_list = []\n",
    "stopword_list = load_security_words()\n",
    "\n",
    "print(\"start nlkt process\")\n",
    "for i, content in enumerate(data):\n",
    "    word_list = tokenize(content)\n",
    "    word_list_cleaned = remove_stopwords(stopword_list, word_list)\n",
    "    keyword_list.extend(word_list_cleaned)\n",
    "\n",
    "year = '2023'\n",
    "start_month = 1\n",
    "end_month = 11\n",
    "data = load_data(year, start_month, end_month)\n",
    "for i, content in enumerate(data):\n",
    "    word_list = tokenize(content)\n",
    "    word_list_cleaned = remove_stopwords(stopword_list, word_list)\n",
    "    keyword_list.extend(word_list_cleaned)\n",
    "    \n",
    "keyword_counter = Counter(keyword_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict = dict(keyword_counter)\n",
    "sorted_data = sorted(word_dict.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sorted_data[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('threat', 4845),\n",
       " ('security', 4693),\n",
       " ('malware', 3693),\n",
       " ('information', 3071),\n",
       " ('vulnerability', 2726),\n",
       " ('victim', 2509),\n",
       " ('ransomware', 2240),\n",
       " ('cybersecurity', 2067),\n",
       " ('Microsoft', 1931),\n",
       " ('campaign', 1828)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
